{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98949a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers in the training dataset\n",
    "#lof = LocalOutlierFactor(random_state=0)  #try other methods    #worse\n",
    "#lof_pred = lof.fit_predict(df1_best)\n",
    "#no_outliers = lof_pred != -1\n",
    "\n",
    "\n",
    "######df1_x_train= df1_best[no_outliers.values, :]\n",
    "\n",
    "\n",
    "#df1_x_train=df1_best.iloc[no_outliers==True]\n",
    "#df1_y_train=df1.iloc[no_outliers==True,[14,15,16]]\n",
    "#df1_y_train\n",
    "\n",
    "#clf = IsolationForest(random_state=0).fit_predict(df1.iloc[:,:10]) #better\n",
    "#df1_x_train = df1.iloc[:,:11][clf==1]\n",
    "#df1_y_train=df1.iloc[clf==1,[11,12,13]]\n",
    "#df1_y_train\n",
    "\n",
    "\n",
    "\n",
    "#col_names=['DEPTH', 'DTC', 'DTS', 'CALI', 'DEN', 'DENC', 'GR', 'NEU', 'PEF',\n",
    "     #   'RDEP', 'RMED','ROP']\n",
    "#col_names=['DEPTH', 'DTC', 'DTS', 'CALI', 'DEN', 'DENC', 'GR', 'NEU', 'PEF',\n",
    "     #   'RDEP', 'ROP']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor\n",
    "col=['DEPTH', 'DTC', 'DTS', 'CALI', 'DEN', 'DENC', 'GR', 'NEU', 'PEF',\n",
    "        'RDEP', 'RMED','PHIF','SW','VSH']\n",
    "\n",
    "strategies = ['ascending']#, 'descending', 'roman', 'arabic', 'random']\n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    imputer=IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=25, random_state=100),imputation_order=s)\n",
    "    # evaluate the model\n",
    "    #cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    #scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # store results\n",
    "df1=df1.loc[:,col]\n",
    "imputer.fit(df1)\n",
    "# transform the dataset\n",
    "df1 = pd.DataFrame(imputer.transform(df1),columns=df1.columns,index=df1.index)\n",
    "# print total missing\n",
    "#print('Missing: %d' % sum(isnan(Xtrans)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df01c6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('dataset_with_iterativeimputer_ExtraTreesRegressor_25estimators.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d1cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('dataset_with_iterativeimputer_ExtraTreesRegressor_25estimators.csv')\n",
    "#df1=df1.reset_index(drop=True)\n",
    "#df1=df1.reset_index(drop=True)\n",
    "df1=df1.drop(columns='Unnamed: 0')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1505adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "\n",
    "# get colormap from seaborn\n",
    "cmap = ListedColormap(sns.color_palette(\"husl\", 256).as_hex())\n",
    "\n",
    "# plot\n",
    "sc = ax.scatter(x, y, z, s=40, c=x, marker='o', cmap=cmap, alpha=1)\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def result_plot(y_predict, y_real, names = ['PHIF', 'SW', 'VSH'], clust_indexes=[],n_points=1000):\n",
    "    RMSE, R2 = [], []\n",
    "    print(names)\n",
    "    for i,name in enumerate(names):\n",
    "        print(i)\n",
    "        if isinstance(name, str)==True:\n",
    "            print(i)\n",
    "            #print(y_real.iloc[:,i])\n",
    "            print(y_predict[i])\n",
    "            RMSE.append(np.sqrt(mean_squared_error(y_real[i], y_predict[i])))\n",
    "            R2.append(r2_score(y_real[i], y_predict[i]))\n",
    "        else:\n",
    "            for j in name:\n",
    "                print(j)\n",
    "                RMSE.append(np.sqrt(mean_squared_error(y_real[i], y_predict[i])))\n",
    "                R2.append(r2_score(y_real[i], y_predict[i]))\n",
    "                \n",
    "        \n",
    "    \n",
    "    # check the accuracy of predicted data and plot the result\n",
    "    y_pred_clusts=pd.DataFrame()\n",
    "    y_true_clusts=pd.DataFrame()\n",
    "    for i,name in enumerate(names):\n",
    "        if isinstance(name, str)==True:\n",
    "            continue\n",
    "        else:\n",
    "            y_pred_clust=[]\n",
    "            y_true_clust=pd.Series([])\n",
    "            for j in name:\n",
    "                y_p=y_predict.pop(i)\n",
    "                y_t=y_real.pop(i)\n",
    "                y_pred_clust=y_pred_clust+y_p\n",
    "                y_true_clust=pd.concat([y_true_clust,y_t],axis=0)\n",
    "                print(f'iter {y_true_clust.size}')\n",
    "            y_pred_clusts=pd.concat([y_pred_clusts,pd.Series(y_pred_clust)],axis=1)\n",
    "            y_true_clusts=pd.concat([y_true_clusts,pd.Series(y_true_clust)],axis=1)\n",
    "            print(f'pred {len(y_pred_clust)},{type(y_p)}')\n",
    "            print(f'true {len(y_true_clust)},{type(y_t)}')\n",
    "    y_predict.append(y_pred_clusts,ignore_index=True)\n",
    "    y_real.append(y_true_clusts,ignore_index=True)\n",
    "    y_predict=pd.DataFrame(y_predict).T\n",
    "    y_real=pd.DataFrame(y_true).T\n",
    "    print(f' {y_real}, {y_predict}')\n",
    "    print('RMSE:', '{:.5f}'.format(np.sqrt(mean_squared_error(y_real, y_predict))))\n",
    "    for j in range(len(RMSE)):\n",
    "        print(f'    {name} : {RMSE[j]:.5f}')\n",
    "#     print(\"-\"*65)\n",
    "    \n",
    "    print( 'R^2: ', r2_score(y_real, y_predict))\n",
    "    for j in range(len(RMSE)):\n",
    "        print(f'    {name} : {R2[j]:.5f}')\n",
    "    \n",
    "    plt.subplots(nrows=3, ncols=2, figsize=(16,16))\n",
    "\n",
    "    for i,name in enumerate(names):   \n",
    "        if isinstance(name, int)==False:           \n",
    "            plt.subplot(3, 2, i*2+1)\n",
    "            plt.plot(y_real[:n_points].iloc[:,i])\n",
    "            plt.plot(y_predict[:n_points, i])\n",
    "            plt.legend(['True', 'Predicted'])\n",
    "            plt.xlabel('Sample')\n",
    "            plt.ylabel(name)\n",
    "            plt.title(name+' Prediction Comparison')\n",
    "       \n",
    "            plt.subplot(3, 2, i*2+2)\n",
    "            plt.scatter(y_real.iloc[:, i], y_predict[:, i], alpha=0.01)\n",
    "            plt.xlabel('Real Value')\n",
    "            plt.ylabel('Predicted Value')\n",
    "            plt.title(name+' Prediction Comparison')\n",
    "                   \n",
    "        else:\n",
    "            for j in name:\n",
    "                plt.subplot(3, 2, i*2+1)\n",
    "                plt.plot(y_real[:n_points].iloc[:,i])\n",
    "                plt.plot(y_predict[:n_points, i])\n",
    "                plt.legend(['True', 'Predicted'])\n",
    "                plt.xlabel('Sample')\n",
    "                plt.ylabel(name)\n",
    "                plt.title(name+' Prediction Comparison')\n",
    "\n",
    "                plt.subplot(3, 2, i*2+2)\n",
    "                plt.scatter(y_real.iloc[:, i], y_predict[:, i], alpha=0.01)\n",
    "                plt.xlabel('Real Value')\n",
    "                plt.ylabel('Predicted Value')\n",
    "                plt.title(name+' Prediction Comparison')\n",
    "                   \n",
    "\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from xgboost import XGBClassifier\n",
    "#param_grid=dict(kmeans__n_clusters=range(2,30)) #2 underscores between the estimator\n",
    "#and the parameters\n",
    "#knn=KNeighborsClassifier(n_neighbors=25)\n",
    "#dbscan=DBSCAN(eps=0.05, min_samples=5)\n",
    "#knn.fit(knn.components_,dbscan.labels_[knn.core_sample_indices_])\n",
    "\n",
    "#dbscan.fit(X_train)\n",
    "#reg = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=100, random_state=100))\n",
    "#pipeline=Pipeline(['model' , Birch(threshold=0.01, n_clusters=n_clust).fit_predict(X_train),\n",
    "                #   ('reg',MultiOutputRegressor(GradientBoostingRegressor(n_estimators=100,\n",
    "                    #                                                     random_state=100)))])\n",
    "\n",
    "#reg = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=100, random_state=100))\n",
    "#reg = MultiOutputRegressor(XGBClassifier(n_estimators=100, random_state=100))\n",
    "#reg = MultiOutputRegressor(Ridge(random_state=100))\n",
    "#reg_best = grid_search(reg, {})\n",
    "\n",
    "'''param_grid={\"max_depth\" : [5],\n",
    "           \"min_samples_leaf\":[1],\n",
    "           \"min_weight_fraction_leaf\":[0.0],\n",
    "           \"max_features\":[\"auto\"],\n",
    "           \"max_leaf_nodes\":[8] ,\n",
    "               'n_estimators': [120]}\n",
    "reg_best_VSH = grid_search(reg, param_grid,X_train, y_train.loc[:,'VSH'],ht='notrandom')'''\n",
    "\n",
    "\n",
    "'''grid = GridSearchCV(estimator=pipeline, \n",
    "                            param_grid=param_grid, \n",
    "                            scoring='r2', \n",
    "                            cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(grid.best_score_)\n",
    "\n",
    "    print(grid.best_estimator_)\n",
    "\n",
    "    #result_plot(reg_best.predict(X_train), y_train)'''\n",
    "\n",
    "\n",
    "\n",
    "'''opt = keras.optimizers.SGD(learning_rate=0.009,nesterov=True)# beta_1=0.9,beta_2=0.99,epsilon=1e-08)\n",
    "model2=keras.models.Sequential([keras.layers.BatchNormalization(),\n",
    "                                   #keras.layers.Dropout(rate=0.05),\n",
    "                                   keras.layers.Dense(1050,activation='selu'\n",
    "                                                      ,kernel_initializer='lecun_normal'\n",
    "                                                     #,kernel_regularizer=keras.regularizers.l2(0.1)\n",
    "                                                     ),\n",
    "                                   #keras.layers.BatchNormalization(),\n",
    "                                   #keras.layers.Dropout(rate=0.07),\n",
    "                                   #keras.layers.Dense(1000,activation='elu',\n",
    "                                                   #   kernel_initializer='glorot_normal',\n",
    "                                                   #  ,kernel_regularizer=keras.regularizers.l2(0.02)\n",
    "                                                     #),\n",
    "                                   keras.layers.BatchNormalization(),\n",
    "                                   #keras.layers.Dropout(rate=0.05),\n",
    "                                   keras.layers.Dense(20,activation='elu'\n",
    "                                                      ,kernel_initializer='glorot_normal'\n",
    "                                                     #,kernel_regularizer=keras.regularizers.l2(0.1)\n",
    "                                                     ),\n",
    "                                   #keras.layers.BatchNormalization(),\n",
    "                                   #keras.layers.Dropout(rate=0.21),\n",
    "                                   keras.layers.Dense(1,activation='sigmoid')])\n",
    "#model.summary()\n",
    "model2.compile(loss='mean_squared_error', optimizer=opt, metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "import os\n",
    "root_logdir=os.path.join(os.curdir,'my_logs')\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id=time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir,run_id)\n",
    "run_logdir=get_run_logdir()\n",
    "#tensorboard_cb=keras.callbacks.TensorBoard(run_logdir)\n",
    "checkpoint_cb=keras.callbacks.ModelCheckpoint('my_keras_model_2_layers_1_iterative_imp_modelSW.h5',save_best_only=True)\n",
    "early_stopping_cb=keras.callbacks.EarlyStopping(patience=66,restore_best_weights=True)\n",
    "\n",
    "history=model2.fit(np.array(X_train),np.array(y_train['SW']),\n",
    "    epochs=1000,validation_data=(X_val,y_val['SW']),\n",
    "                     callbacks=[checkpoint_cb,early_stopping_cb])\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e75c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result on validation dataset only\n",
    "'''yhat_val = model.predict(X_val)\n",
    "clusters_val = unique(yhat_val)\n",
    "clusts_val=[]\n",
    "for i in range(n_clust):\n",
    "    clust_val=np.where(yhat_val==i)\n",
    "    clusts_val.append(clust_val) # dataframes nested into a list\n",
    "    #print(clusts_val)\n",
    "for c in clusts_val:\n",
    "    try:\n",
    "        #print(X_val.iloc[c])\n",
    "         #reg_best = grid_search(reg, {},X_train.iloc[c],y_train.iloc[c])\n",
    "        result_plot(reg_best.predict(X_val.iloc[c]), y_val.iloc[c])\n",
    "    except ValueError:\n",
    "        continue\n",
    "    except IndexError:\n",
    "        continue'''\n",
    "\n",
    "#gradient boosting\n",
    "#0.23764 isolation forest, iterative inputer k=8\n",
    "#0.067 k=11 isolation forest, iterative inputer\n",
    "#0.08297 k=12 isolation forest, iterative inputer\n",
    "#0.04202 k=11 no automatic filling missing values, isolation forest    \n",
    "#0.05562 k=11 no automatic filling missing values, isolation forest , 3 clusters on VSH feature\n",
    "#0.07920 k=11 no aut filling, isolation forest , 3 clusters on VSH feature, extratrees first cluster\n",
    "#0.05768 k=11 no aut filling, isolation forest , 3 clusters on VSH feature, adaboost first cluster\n",
    "\n",
    "\n",
    "# 0.06588 k=11 no automatic filling missing values, isolation forest , ExtraTrees VSH feature\n",
    "# 0.04201 k=11 no automatic filling missing values, isolation forest , ExtraTrees PHIF feature \n",
    "#0.05880 k=11 no automatic filling missing values, isolation forest , ExtraTrees PHIF and SW feature \n",
    "#0.04282 k=11 no aut filling, isolation forest , ExtraTrees PHIF, adaboost VSH feature \n",
    "#0.06766 k=11 no aut filling, isolation forest , ExtraTrees PHIF, adaboost SW feature \n",
    "# 0.04114 no automatic filling missing values, isolation forest , ExtraTrees PHIF feature                                \n",
    "#VSH GradientBoostingRegressor(max_depth=5, max_features='auto', max_leaf_nodes=8,  \n",
    "                          #min_samples_leaf=2, n_estimators=110,min_weight_fraction_leaf= 0.0,\n",
    "                          #random_state=100)\n",
    "#0.04023  no automatic filling missing values, isolation forest , ExtraTrees PHIF feature      \n",
    "#VSH GradientBoostingRegressor(max_depth=5, max_features='auto', max_leaf_nodes=8,\n",
    "#                          n_estimators=120, random_state=100)       \n",
    "        \n",
    "\n",
    "\n",
    "#no automatic filling missing values, isolation forest , ExtraTrees PHIF feature \n",
    "#RMSE: 0.03838\n",
    "   # PHIF  : 0.00498\n",
    "   # SW    : 0.03477\n",
    "   # VSH   : 0.05643\n",
    "#R^2:  0.9507901801163379\n",
    " #   PHIF  : 0.99239\n",
    " #   SW    : 0.92284\n",
    " #   VSH   : 0.93714\n",
    "#VSH  GradientBoostingRegressor(max_depth=5, max_features='auto', max_leaf_nodes=8,\n",
    "     #  n_estimators=120, random_state=100)\n",
    "\n",
    "# PHIF ExtraTreesRegressor(random_state=100)\n",
    "\n",
    "#  SW  GradientBoostingRegressor(max_depth=8, max_features='auto', max_leaf_nodes=9,\n",
    "            #n_estimators=80, random_state=100)\n",
    " \n",
    " #no automatic filling missing values, isolation forest , ExtraTrees PHIF feature    \n",
    "#RMSE: 0.03782\n",
    "   # PHIF  : 0.00498\n",
    "    #SW    : 0.03477\n",
    "   # VSH   : 0.05530\n",
    "#R^2:  0.9516247800858518\n",
    "  #  PHIF  : 0.99239\n",
    "  #  SW    : 0.92284\n",
    "  #  VSH   : 0.93965 \n",
    "\n",
    "   \n",
    "#VSH #learning_rate=0.0000014 Adam, elu, glorot, 950_50 ,BatchNorm\n",
    "\n",
    "# PHIF ExtraTreesRegressor(random_state=100)\n",
    "\n",
    "#  SW  GradientBoostingRegressor(max_depth=8, max_features='auto', max_leaf_nodes=9,\n",
    "            #n_estimators=80, random_state=100) \n",
    "\n",
    "#RMSE: 0.03622\n",
    "  #  PHIF  : 0.00498\n",
    "  #  SW    : 0.03477\n",
    "  #  VSH   : 0.05198\n",
    "#R^2:  0.9539654169255757\n",
    "  #  PHIF  : 0.99239\n",
    "  #  SW    : 0.92284\n",
    "  #  VSH   : 0.94667\n",
    "#VSH #learning_rate=0.0000014 Adam, elu, glorot, 1250_50 ,BatchNorm\n",
    "\n",
    "    \n",
    "\n",
    "#RMSE: 0.03424\n",
    "#    PHIF  : 0.00488\n",
    "#    SW    : 0.03416\n",
    "#    VSH   : 0.04823\n",
    "#R^2:  0.9574370087896602\n",
    "#    PHIF  : 0.99270\n",
    "#    SW    : 0.92554\n",
    "#    VSH   : 0.95408\n",
    "#VSH #learning_rate=0.0000014 Adam, beta_1=0.9,beta_2=0.98,epsilon=1e-08,\n",
    "#elu, glorot, 1250_50 ,BatchNorm    \n",
    "#GradientBoostingRegressor(max_depth=7, max_features='auto', max_leaf_nodes=9,\n",
    "                         # random_state=120)    \n",
    "# ExtraTreesRegressor(max_depth=10,n_estimators=300)   \n",
    "\n",
    "                          \n",
    "#'my_keras_model_13bis_best.h5'    \n",
    "    \n",
    "#RMSE: 0.03356\n",
    " #   PHIF  : 0.00488\n",
    " #   SW    : 0.03416\n",
    " #   VSH   : 0.04678\n",
    "#R^2:  0.9583468874779874\n",
    "#    PHIF  : 0.99270\n",
    "#    SW    : 0.92554\n",
    " #   VSH   : 0.95681    \n",
    "#VSH #learning_rate=0.0000024 Adam, beta_1=0.9,beta_2=0.98,epsilon=1e-08, amsgrad='False'\n",
    "#elu, glorot, 1250_50 ,BatchNorm    \n",
    "#SW GradientBoostingRegressor(max_depth=7, max_features='auto', max_leaf_nodes=9,\n",
    "                         # random_state=120)    \n",
    "#PHIF ExtraTreesRegressor(max_depth=10,n_estimators=300)   \n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#0.04302  k=10 no authomatic filling missing values, isolation forest\n",
    "#0.04404 k=12 no authomatic filling missing values, isolation forest\n",
    "\n",
    "#0.08  k=11,clusters=11 no authomatic filling missing values, isolation forest\n",
    "#0.088  k=20,clusters=11 no authomatic filling missing values, isolation forest\n",
    "\n",
    "#0.08296 k=12\n",
    "#0.08144 k=10\n",
    "#ridge_regression 0.132\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
